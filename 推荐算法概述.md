# 推荐算法概述

推荐系统的目标是向user推荐他可能感兴趣的item。在早期，推荐问题常常被看做评分预测问题，即已知user对一组item的评分，预测他对一个新的item的评分。著名的netflix竞赛就是属于这种类型。近年来推荐系统更侧重于topN推荐，即向user推荐他最可能感兴趣的N个items。topN推荐在实际生活中的应用更广泛，本文所针对的都是topN推荐。

推荐系统整体上可以分为前端和后端两大部分。前端包括App或网页上呈献给用户的推荐位、推荐条目信息，以及对用户点击浏览行为的埋点记录；后端包括推荐算法和推荐效果评估。前端和后端会形成闭环：推荐算法->app上对用户呈现->用户行为的记录和反馈->推荐效果评估->改进推荐算法。

本文重点讲述推荐系统后端当中的推荐算法环节。

从功能角度，推荐算法可以分为两大部分：召回(Recall)和排序（Rank）。所谓召回，是指从item全集中筛选出推荐候选集；而排序则是指对候选集中的每一个item进行匹配度打分，将items按照匹配度得分进行降序排列。

在十年前，人们对于推荐算法的研究侧重于召回算法。常见的召回算法有协同过滤、隐因子模型、基于图的召回算法等等。近十年随着手机成为最主要的信息传播媒介，同时不同应用之间的竞争愈发激烈，为了进一步挖掘推荐系统的性能，业界对排序算法的关注和研究也在逐渐增加。

下面分别从召回和排序两个方面介绍推荐算法。

## 常用的召回算法

### UserCF 
即基于User的协同过滤（Collaborative Filtering）。UserCF的思路是向目标用户推荐跟他兴趣相似的用户所喜欢的item。所以UserCF通常包含两个过程：
#### 1.找到跟目标用户兴趣相似的用户集合
#### 2.找到该集合中的用户喜欢的，而目标用户尚未接触过的item并推荐给目标用户

用户之间的similarity是通过用户的行为统计得到的。这里的行为指的是用户对item的点击、浏览、购买、评分等行为（通常点击、浏览被称为隐性行为，这类行为携带信息量小，但由于发生频率高，数据量大，因而也具有统计价值；购买、评分被称为显性行为，这类行为更直接反映了用户喜好，但是发生频次较低，数据规模较小）。例如基于点击行为计算用户的Jaccard similarity：
>* 有两个用户user1和user2，他们各自点击过的item集合分别是S1和S2，那么user1和user2的similarity可以定义为S1和S2交集的模除以S1和S2并集的模。

similarity的定义可以有多种方式，除了上例采用的Jaccard similarity之外，还可以采用Cosine similarity，Pearson similarity等等。

user CF的计算方式如下（以点击行为为例）：
>* 每个数据样本形如(user, item)，即某个user点击了某个item。为简便，同一个user对同一个item的多次点击只算做一次。即点击行为为按bool值计算，true代表点击过（无论点击了多少次），false代表没点击过
>* 假设有m个user，n个item，总点击数为N，即每个item平均被N/n个user点击过，每个user平均点击了N/m个item
>* 首先要得到字典形式的item-user关系表dict={item:users}，dict的key是每一个item，value是点击过这个item的所有user的列表
>* 然后遍历item-user关系表求得user-user相似度矩阵U。矩阵U为m * m维，U每一个元素Uij代表第user-i和user-j共同点击过的item总个数
>* 计算字典形式的user-item关系表dict={user:items}，dict的key是每一个user，value是这个user点击过的所有item的列表
>* 得到U和user-item关系表之后，就可以方便的求出任意目标user的相似user集合，然后推荐这个集合当中的user所点击过的item给目标user
>* 从上可以看出，要想给一个user做推荐需要先计算得到user-user相似度矩阵U和user-item关系表。建立矩阵和关系表的时间复杂度都为O(N)，空间复杂度分别为O(m * m)和O(N)

UserCF的可解释性较好，推荐理由可以是“跟你兴趣类似的人也喜欢”这种。

### ItemCF
ItemCF的思路是给用户推荐跟他之前喜欢的item类似的item。对于一个item i，找到与其相似的一个item集合I，然后把I推荐给喜欢i的人。这里item之间的相似度是通过user对它们的行为来判定的，itemCF假设，如果有很多人同时喜欢item a和item b，那么说明a和b具有较高的similarity。

itemCF的计算类似userCF，只不过需要计算的是item-item相似度矩阵而非user-user相似度矩阵。itemCF优于userCF的地方：

>* item-item相似度比user-user相似度更稳定，因为物品的属性相对规定，行为分布也相对稳定（比如90%的好评率），而人的口味会变，行为随机性强
>* 同一个用户可能只有几十个几百个行为记录，同一个item有几万几十万行为记录；因此基于item算相似度更准确
>* 用户数目可能几千万甚至上亿，而同类型的item种类少得多，因此item-item相关矩阵比user-user相关矩阵小得多，基于item算相似度更经济
>* 由于item特性相对固定，可以预先计算item-item相关矩阵
>* itemCF的可解释性更好，推荐理由可以是”类似商品“、“喜欢a的人也喜欢”、”购买了a的人也买了“这种

itemCF也有缺点，比如缺乏新颖性，时常会推荐一些重复雷同的商品。

itemCF和userCF有很多共同的特点，比如都没有学习过程，不需要迭代；都需要遍历样本集，构造相似度矩阵和相关表；都不需要user和item的属性特征；都可以根据新增样本进行增量更新；都有冷启动的问题；都需要大量的数据样本才能捕获user或item的相似度关系并得到较好的推荐效果。

### 隐语义模型（Latent Factor Model）
在itemCF或userCF中，item和user都被表征为高维one-hot vector。one-hot vector是无法直接计算similarity的，因而itemCF和userCF都采用了”曲线救国“的方式计算similarity：在user空间计算item的similarity，在item空间计算user的similarity，两个空间通过用户的行为联系起来。隐语义模型采用了不同的思路，它借鉴了NLP中的embedding的思想，把item和user同时从高维稀疏one-hot vector转化为低维稠密latent vector，相当于把item空间和user空间投影到同一个低维度latent空间，从而可以直接计算item-item、user-user、item-user的similarity。
在隐语义模型中，每个训练样本形如：(user, item, label)。label既可以是bool即{点击过，没点过}，也可以是int即numOfClicks。隐语义模型的训练过程如下：
>* 首先将每个user和每个item都随机初始化为K维向量即latent factor，这里K是超参数，即latent factor的维度，一般是几十，通常不超过100。模型的参数即所有user和所有item的latent factor。
>* 开始迭代。每一轮迭代中，对于每一个训练样本，用user latent factor和item latent factor的内积去逼近label。迭代通常采用SGD的方式来调整参数降低loss。
>* 迭代结束时，算法最终输出就是所有user latent factor和item latent factor。任意两个user或item的相似度都可以从对应latent factor的内积得到。

假设总共有N个训练样本，m个user，n个item，迭代L次，那么时间复杂度为O（L * N * K），空间复杂度为O（m * K + n * K）。

也可以从矩阵分解的角度理解隐语义模型。user和item的行为关系可以用m * n维的矩阵R表示，Rij表示第i个user对第j个item的行为。隐语义模型试图将user表征成m * K维矩阵M，item表征成n * K维矩阵N，并用M * N逆去逼近R。 

隐语义模型有很多变体，例如spark.ml里的ALS算法（Alternating Least Squares）就是隐语义模型的一种分布式实现。

隐语义模型可以方便的找到每个用户的topN like，item的topN similar，然后离线写入数据库或内存，以便后续在线查找。由于不需要user或item相关矩阵，隐语义模型对存储的需求远小于itemCF或userCF。

隐语义模型的缺点是可解释性较差；另外对于新增的样本没办法增量计算，只能从头批量计算。

### Personal Rank
Personal Rank是一种基于二分图的模型，该模型的提出是受到了著名的Page Rank算法的启发。图中有两类顶点，分别是user顶点和item顶点，图中的每一条边都连接了一个user顶点和一个item顶点。如下图，黑色圆代表user，白色方框代表item，user和item之间的边代表该user对该item产生过行为。
![Image text](https://github.com/pengxiaoo/recommender-system/blob/master/imgs/personal_rank.png)
在Personal Rank算法中，找到一个user的topN like，就是找到跟user不直接相连，并且相关度最高的N个item顶点。
图中顶点的相关度主要取决与以下因素： 
>* 两个顶点之间路径数(越多越好) 
>* 两个顶点之间路径长度(越短越好) 
>* 两个顶点之间路径经过的顶点的出度(越少越好) 

Personal Rank通过随机游走的方式来确定user顶点和item顶点之间的相关度。假设给用户A进行个性化推荐，从图中某个user顶点VA开始游走，游走到一个节点时，首先按照概率alpha决定是否继续游走，还是停止这次游走并直接返回VA顶点开始重新游走。如果决定继续游走，那么就从当前顶点出度中的顶点中按照均匀分布随机选择一个作为下次经过的顶点。这样经过很多次的随机游走后，每个item顶点被访问到的概率就会收敛，这个概率就是该item和user的相关度。最终推荐列表中物品的权重就是item节点的访问概率。上述描述可以用公式表示为：
![Image text](https://github.com/pengxiaoo/recommender-system/blob/master/imgs/personal_rank_formula.png)
上图中，PR(v)代表顶点v的访问概率。in(v)和out(v)分别代表顶点v的入度和出度。

Personal Rank对每一个user进行推荐都要全图迭代，比较耗时。工业界应用时通常是采用矩阵运算方式，对所有user同时进行迭代。

注：上面的示例图来自于项亮的《推荐系统实践》一书。

### Item2Vec



