# 推荐算法概述

推荐系统整体上可以分为前端和后端两大部分。前端包括App或网页上呈献给用户的推荐位、推荐条目信息，以及对用户点击浏览行为的埋点记录；后端包括推荐算法和推荐效果评估。前端和后端会形成闭环：推荐算法->app上对用户呈现->用户行为的记录和反馈->推荐效果评估->改进推荐算法。

本文重点讲述推荐系统后端当中的推荐算法环节。

从功能角度，推荐算法可以分为两大部分：召回(Recall)和排序（Rank）。所谓召回，是指从item全集中筛选出推荐候选集；而排序则是指对候选集中的每一个item进行匹配度打分，将items按照匹配度得分进行降序排列。

在十年前，人们对于推荐算法的研究侧重于召回算法。常见的召回算法有协同过滤、隐因子模型、基于图的召回算法等等。近十年随着手机成为最主要的信息传播媒介，同时不同应用之间的竞争也愈发激烈，为了进一步挖掘推荐系统的性能，业界对排序算法的关注和研究在逐渐增加。

下面分别从召回和排序两个方面介绍推荐算法。

## 常用的召回算法

### UserCF 
即基于User的协同过滤（Collaborative Filtering）。UserCF的思路是向目标用户推荐跟他兴趣相似的用户所喜欢的item。所以UserCF通常包含两个过程：
#### 1.找到跟目标用户兴趣相似的用户集合
#### 2.找到该集合中的用户喜欢的，而目标用户尚未接触过的item并推荐给目标用户

用户之间的similarity是通过用户的行为统计得到的。这里的行为指的是用户对item的点击、浏览、购买、评分等行为（通常点击、浏览被称为隐性行为，这类行为携带信息量小，但由于发生频率高，数据量大，因而也具有统计价值；购买、评分被称为显性行为，这类行为更直接反映了用户喜好，但是发生频次较低，数据规模较小）。例如基于点击行为计算用户的Jaccard similarity：
>* 有两个用户user1和user2，他们各自点击过的item集合分别是S1和S2，那么user1和user2的similarity可以定义为S1和S2交集的模除以S1和S2并集的模。

similarity的定义可以有多种方式，除了上例采用的Jaccard similarity之外，还可以采用Cosine similarity，Pearson similarity等等。

user CF的计算方式如下（以点击行为为例）：
>* 每个数据样本形如(user, item)，即某个user点击了某个item。为简便，同一个user对同一个item的多次点击只算做一次。即点击行为为按bool值计算，true代表点击过（无论点击了多少次），false代表没点击过
>* 假设有m个user，n个item，总点击数为N，即每个item平均被N/n个user点击过，每个user平均点击了N/m个item
>* 首先要得到字典形式的item-user关系表dict={item:users}，dict的key是每一个item，value是点击过这个item的所有user的列表
>* 然后遍历item-user关系表求得user-user相似度矩阵U。矩阵U为m * m维，U每一个元素Uij代表第user-i和user-j共同点击过的item总个数
>* 计算字典形式的user-item关系表dict={user:items}，dict的key是每一个user，value是这个user点击过的所有item的列表
>* 得到U和user-item关系表之后，就可以方便的求出任意目标user的相似user集合，然后推荐这个集合当中的user所点击过的item给目标user
>* 从上可以看出，要想给一个user做推荐需要先计算得到user-user相似度矩阵U和user-item关系表。建立矩阵和关系表的时间复杂度都为O(N)，空间复杂度分别为O(m * m)和O(N)

UserCF的可解释性较好，推荐理由可以是“跟你兴趣类似的人也喜欢”这种。

### ItemCF
ItemCF的思路是给用户推荐跟他之前喜欢的item类似的item。对于一个item i，找到与其相似的一个item集合I，然后把I推荐给喜欢i的人。这里item之间的相似度是通过user对它们的行为来判定的，itemCF假设，如果有很多人同时喜欢item a和item b，那么说明a和b具有较高的similarity。

itemCF的计算类似userCF，只不过需要计算的是item-item相似度矩阵而非user-user相似度矩阵。itemCF优于userCF的地方：

>* item-item相似度比user-user相似度更稳定，因为物品的属性相对规定，行为分布也相对稳定（比如90%的好评率），而人的口味会变，行为随机性强
>* 同一个用户可能只有几十个几百个行为记录，同一个item有几万几十万行为记录；因此基于item算相似度更准确
>* 用户数目可能几千万甚至上亿，而同类型的item种类少得多，因此item-item相关矩阵比user-user相关矩阵小得多，基于item算相似度更经济
>* 由于item特性相对固定，可以预先计算item-item相关矩阵
>* itemCF的可解释性更好，推荐理由可以是”类似商品“、“喜欢a的人也喜欢”、”购买了a的人也买了“这种

itemCF也有缺点，比如缺乏新颖性，时常会推荐一些重复雷同的商品。

itemCF和userCF有很多共同的特点，比如都没有学习过程，不需要迭代；都需要遍历样本集，构造相似度矩阵和相关表；都不需要人工标记数据和复杂的特征工程；都可以根据新增样本进行增量更新；都有冷启动的问题；都需要大量的数据样本才能捕获user或item的相似度关系并得到较好的推荐效果。

### 隐语义模型（Latent Factor Model）
在itemCF或userCF中，item和user都被表征为高维one-hot vector。one-hot vector是无法直接计算similarity的，因而itemCF和userCF都采用了”曲线救国“的方式计算similarity。隐语义模型借鉴了NLP中的embedding的思想，把item和user从高维度稀疏one-hot vector转化为低维度稠密latent vector，从而可以直接计算similarity。
在隐语义模型中，每个训练样本形如：(user, item, label)。label既可以是bool即{点击过，没点过}，也可以是int即numOfClicks。隐语义模型的训练过程如下：
>* 首先将每个user和每个item都随机初始化为K维向量即latent factor，这里K是超参数，即latent factor的维度，一般是几十，通常不超过100。模型的参数即所有user和所有item的latent factor。
>* 开始迭代。每一轮迭代中，对于每一个训练样本，用user latent factor和item latent factor的内积去逼近label。迭代通常采用SGD的方式来调整参数降低loss。
>* 迭代结束时，算法最终输出就是所有user latent factor和item latent factor。任意两个user或item的相似度都可以从对应latent factor的内积得到。

假设总共有N个训练样本，m个user，n个item，迭代L次，那么时间复杂度为O（L * N * K），空间复杂度为O（m * K + n * K）。

隐语义模型可以方便的找到每个用户的topN like，item的topN similar，然后离线写入redis或内存，以便后续在线查找。由于不需要user或item相关矩阵，隐语义模型对存储的需求远小于itemCF或userCF。

隐语义模型的缺点是对于新增的样本没办法增量计算，只能从头批量计算。
