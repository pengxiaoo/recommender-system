# 推荐效果评估

## 离线评估
离线评估是采用监督学习的方式，基于从用户行为日志中构建的训练和测试样本，通过一些预先定义好的离线评估指标，对推荐算法进行训练和验证。离线评估有很多优点：不需要将算法部署到生产环境当中，也不需要用户参与，并且很快能看到结果。

### 评估指标
召回算法和排序算法有不同的离线评估指标。[学习排序简介][1]中已经对排序算法中常用的评估指标作了介绍。这里主要介绍召回算法的评估指标。

#### Precision(准确率)
即召回的所有item当中，有多大比例是用户真正感兴趣的（用户产生了点击、浏览等行为）？
#### Recall(召回率)
即用户感兴趣的所有item当中，有多大比例被召回？

准确率和召回率都是越高越好，但是它们是此消彼长的关系。极限情况下，假如所有的item都被召回，那么召回率就是100%，但是准确率会接近0。
#### AUC-ROC
AUC-ROC在[学习排序简介][1]中已有介绍。AUC-ROC的独特之处是不仅可以评估排序算法，很多召回算法也可以用它来评估。
#### Coverage(覆盖度)
即被推荐的商品个数占总商品个数的比例。跟前面几种评估指标不同，覆盖度体现了商品供应商的立场。理想情况下，希望所有商品都有被推荐出去的机会，而不是仅限若干热门商品。

除了上面提到的几种评估指标之外，还有推荐商品的多样性、新颖度等指标。这些指标的详细描述可以参考项亮的《推荐系统实践》。

### 训练和测试集的构建
由于离线评估是采用监督学习的方式实现的，因此需要构造标签数据，以供推荐算法进行训练和交叉验证。标签数据通常需要结合前端打点来构造，即在app/web上记录用户的点击浏览行为，记录下三元组(userId,itemId,label)，回传给服务器，作为标签数据记录在日志系统当中。

根据推荐商品的类型以及推荐算法的不同，上面提到的label既可以是布尔值（代表发生过点击），也可以是浏览时长。

#### 负样本采样
对于一个特定的用户，我们通过前端打点的方式搜集到的标签数据通常都是正样本（即该用户对该商品发生过点击浏览行为）。但是监督学习中，只有正样本是不够的。如何构建该用户的负样本——即没有发生过行为的标签数据？通常的做法是采样满足下面两个条件的样本，作为该用户的负样本：
>* 该商品被充分曝光过，并被较多的其他用户点击浏览过；
>* 该商品展示给了该用户，但是却没有得到点击浏览。

直观上理解，满足上面两个条件的样本，说明用户确实对该商品缺乏兴趣，因而可以作为负样本。

## 在线评估


### 评估指标

### ab-test

[1]: https://github.com/pengxiaoo/recommender-system/blob/master/docs/rank.md
